#!/usr/bin/env python3
"""
Script maestro para entrenar m√∫ltiples algoritmos de ML y compararlos.

Este script orquesta el pipeline completo de experimentaci√≥n multi-modelo,
desde el procesamiento de datos hasta la comparaci√≥n y persistencia de todos
los algoritmos entrenados. Reemplaza el train_model.py con capacidades avanzadas.
"""

import logging
import sys
import argparse
from pathlib import Path
import time

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_argument_parser():
    """Crear parser de argumentos de l√≠nea de comandos."""
    parser = argparse.ArgumentParser(
        description="Entrenar y comparar m√∫ltiples algoritmos de ML para clasificaci√≥n de ping√ºinos"
    )
    
    parser.add_argument(
        '--algorithms', 
        nargs='*',
        choices=['logistic_regression', 'random_forest', 'gradient_boosting', 'svm_rbf', 'neural_network'],
        help='Algoritmos espec√≠ficos a entrenar. Si no se especifica, entrena todos.'
    )
    
    parser.add_argument(
        '--models-dir',
        type=str,
        default='models',
        help='Directorio donde guardar los modelos (default: models)'
    )
    
    parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help='Proporci√≥n de datos para testing (default: 0.2)'
    )
    
    parser.add_argument(
        '--cv-folds',
        type=int,
        default=5,
        help='N√∫mero de folds para validaci√≥n cruzada (default: 5)'
    )
    
    parser.add_argument(
        '--no-auto-activate',
        action='store_true',
        help='No activar autom√°ticamente el mejor modelo'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Mostrar informaci√≥n detallada de entrenamiento'
    )
    
    return parser


def print_results_summary(comparison_results):
    """Imprimir resumen de resultados de manera legible."""
    print("\n" + "="*80)
    print("üèÜ RESULTADOS DEL ENTRENAMIENTO MULTI-MODELO")
    print("="*80)
    
    # Informaci√≥n del experimento
    exp_info = comparison_results.get("experiment_info", {})
    print(f"üìä Total de modelos entrenados: {exp_info.get('total_models_trained', 0)}")
    print(f"üèÖ Mejor modelo: {exp_info.get('best_model', 'N/A')}")
    print(f"‚è±Ô∏è  Timestamp: {exp_info.get('timestamp', 'N/A')}")
    print(f"üîÑ CV folds: {exp_info.get('cv_folds', 'N/A')}")
    print(f"üìù Test size: {exp_info.get('test_size', 'N/A')}")
    
    # Ranking de modelos
    print(f"\nü•á RANKING POR ACCURACY:")
    ranking = comparison_results.get("ranking", [])
    for rank_info in ranking:
        rank = rank_info.get("rank", 0)
        model = rank_info.get("model", "unknown")
        accuracy = rank_info.get("accuracy", 0.0)
        medal = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else f"{rank}."
        print(f"   {medal} {model}: {accuracy:.4f}")
    
    # Detalles por modelo
    print(f"\nüìà M√âTRICAS DETALLADAS:")
    models_summary = comparison_results.get("models_summary", {})
    
    for model_name, summary in models_summary.items():
        is_best = "üëë" if summary.get("is_best", False) else "  "
        algorithm = summary.get("algorithm", "unknown")
        test_acc = summary.get("test_accuracy", 0.0)
        cv_acc = summary.get("cv_accuracy", 0.0)
        cv_std = summary.get("cv_std", 0.0)
        precision = summary.get("precision", 0.0)
        recall = summary.get("recall", 0.0)
        f1 = summary.get("f1_score", 0.0)
        time_s = summary.get("training_time", 0.0)
        
        print(f"\n{is_best} {model_name.upper()} ({algorithm})")
        print(f"   üìä Test Accuracy:    {test_acc:.4f}")
        print(f"   üîÑ CV Accuracy:      {cv_acc:.4f} ¬± {cv_std:.4f}")
        print(f"   üéØ Precision:        {precision:.4f}")
        print(f"   üîç Recall:           {recall:.4f}")
        print(f"   ‚öñÔ∏è  F1-Score:         {f1:.4f}")
        print(f"   ‚è±Ô∏è  Training Time:    {time_s:.2f}s")
    
    print("\n" + "="*80)


def main():
    """Funci√≥n principal que ejecuta el pipeline multi-modelo."""
    # Parsear argumentos
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # Configurar logging verbose si se solicita
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    start_time = time.time()
    
    try:
        logger.info("=== INICIANDO PIPELINE MULTI-MODELO ===")
        logger.info(f"Directorio de modelos: {args.models_dir}")
        logger.info(f"Test size: {args.test_size}")
        logger.info(f"CV folds: {args.cv_folds}")
        logger.info(f"Algoritmos: {args.algorithms if args.algorithms else 'todos'}")
        
        # Importar m√≥dulos despu√©s de configurar logging
        from src.data_processing import process_penguins_data
        from src.multi_model_trainer import MultiModelTrainer
        from src.model_manager import MultiModelManager
        
        # Paso 1: Procesar datos
        logger.info("üìä Paso 1: Procesando datos Palmer Penguins...")
        X, y, processing_info = process_penguins_data()
        logger.info(f"Datos procesados: {len(X)} muestras, {len(X.columns)} features")
        
        # Paso 2: Configurar entrenador multi-modelo
        logger.info("ü§ñ Paso 2: Configurando entrenador multi-modelo...")
        trainer = MultiModelTrainer(
            test_size=args.test_size,
            cv_folds=args.cv_folds
        )
        
        # Filtrar algoritmos si se especificaron
        if args.algorithms:
            available_algorithms = list(trainer.model_configs.keys())
            invalid_algorithms = set(args.algorithms) - set(available_algorithms)
            
            if invalid_algorithms:
                logger.error(f"Algoritmos no v√°lidos: {list(invalid_algorithms)}")
                logger.error(f"Algoritmos disponibles: {available_algorithms}")
                sys.exit(1)
            
            # Filtrar configuraciones
            original_count = len(trainer.model_configs)
            trainer.model_configs = {name: config for name, config in trainer.model_configs.items() 
                                   if name in args.algorithms}
            
            logger.info(f"Filtrado: {len(trainer.model_configs)}/{original_count} algoritmos seleccionados")
        
        # Paso 3: Entrenar todos los modelos
        logger.info(f"üöÄ Paso 3: Entrenando {len(trainer.model_configs)} algoritmos...")
        comparison_results = trainer.train_all_models(X, y)
        
        # Paso 4: Guardar artefactos
        logger.info("üíæ Paso 4: Guardando artefactos...")
        model_manager = MultiModelManager(models_dir=args.models_dir)
        
        save_result = model_manager.save_multiple_models(
            trainer.trained_models,
            trainer.scaler,
            comparison_results
        )
        
        logger.info("Artefactos guardados:")
        for artifact_type, path in save_result["files_saved"].items():
            logger.info(f"  üìÑ {artifact_type}: {path}")
        
        # Paso 5: Activar el mejor modelo (si se solicita)
        best_model = comparison_results.get("experiment_info", {}).get("best_model")
        
        if not args.no_auto_activate and best_model:
            logger.info(f"üëë Paso 5: Activando mejor modelo: {best_model}")
            model_manager.set_active_model(best_model)
        else:
            logger.info("‚è≠Ô∏è  Paso 5: Saltado - no se activ√≥ autom√°ticamente el mejor modelo")
        
        # Mostrar resultados
        if not args.verbose:
            print_results_summary(comparison_results)
        
        # Estad√≠sticas finales
        total_time = time.time() - start_time
        
        logger.info("=== PIPELINE MULTI-MODELO COMPLETADO ===")
        logger.info(f"üèÜ Mejor modelo: {best_model}")
        logger.info(f"‚è±Ô∏è  Tiempo total: {total_time:.2f} segundos")
        logger.info(f"üíæ Modelos guardados en: {args.models_dir}")
        
        if not args.no_auto_activate and best_model:
            logger.info(f"‚úÖ Modelo {best_model} est√° activo y listo para predicciones")
        
    except Exception as e:
        logger.error(f"‚ùå Error en pipeline multi-modelo: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
